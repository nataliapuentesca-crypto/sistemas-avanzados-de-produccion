{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliapuentesca-crypto/sistemas-avanzados-de-produccion/blob/main/NataliaPuentes_DennisCarrillo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taller 3: Regresión Lineal y Conceptos Fundamentales\n",
        "\n"
      ],
      "metadata": {
        "id": "GntjqrqtVmM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instrucciones Generales\n",
        "\n",
        "**Fecha de entrega:** 20 de septiembre.\n",
        "**Tiempo disponible:** Tienen dos clases y el fin de semana para completar el taller.\n",
        "\n",
        "1.  **Trabajo en Grupo:** Este taller se debe realizar en los grupos previamente definidos.\n",
        "2.  **Repositorio en GitHub:** Cada grupo debe crear un repositorio en GitHub.\n",
        "    * Una sola persona del grupo es responsable de crearlo.\n",
        "    * El repositorio debe llamarse **`Regresion_Lineal`**.\n",
        "    * Este archivo debe ser subido al repositorio con el nombre **`Regresion_Lineal.ipynb`**.\n",
        "    * **Alternativa:** Si tienen complicaciones con GitHub, pueden enviar el enlace del cuaderno de Google Colab al aula virtual.\n",
        "3.  **Actividad a Mano (20% de la nota):**\n",
        "    * La asistencia a clase es **obligatoria**.\n",
        "    * Se realizará una actividad práctica de regresión lineal a mano.\n",
        "    * Cada integrante del grupo deberá firmar la hoja de la actividad, la cual constituye el 20% de la nota de este taller.\n",
        "    "
      ],
      "metadata": {
        "id": "vBKCWxfzUoMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 1: Definición del Problema**\n",
        "\n",
        "Piensa en tu experiencia académica o laboral en un área de operaciones y elige **un problema** relacionado con inventarios, calidad, mantenimiento o productividad.\n",
        "\n",
        "1.  Indica cuál es tu variable dependiente **$Y$** (aquello que quieres predecir o clasificar).\n",
        "2.  Lista entre 3 y 5 variables independientes **$X$** que medirías para predecir $Y$.\n",
        "3.  Especifica si tu problema es de **regresión** (predecir un valor numérico) o de **clasificación**."
      ],
      "metadata": {
        "id": "ueGQVZ8FU2QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Problema Seleccionado: Predicción de Defectos en Línea de Producción\n",
        "Contexto:\n",
        "En una planta de manufactura de componentes electrónicos, es crítico predecir cuándo los productos tendrán defectos para optimizar el control de calidad y reducir costos de reproceso.\n",
        "\n",
        "1. Variable Dependiente (Y):\n",
        "Y = Tasa de Defectos por Lote\n",
        "\n",
        "Definición: Porcentaje de productos defectuosos en cada lote de producción\n",
        "Unidad: Porcentaje (0% - 100%)\n",
        "Ejemplo: Si un lote de 1,000 unidades tiene 25 defectuosas, Y = 2.5%\n",
        "\n",
        "\n",
        "2. Variables Independientes (X):\n",
        "X₁ = Temperatura del Proceso de Soldadura\n",
        "\n",
        "Unidad: Grados Celsius (°C)\n",
        "Rango típico: 220°C - 260°C\n",
        "Justificación: Temperaturas fuera del rango óptimo afectan la calidad de las soldaduras\n",
        "\n",
        "X₂ = Humedad Relativa del Ambiente\n",
        "\n",
        "Unidad: Porcentaje (%)\n",
        "Rango típico: 40% - 65%\n",
        "Justificación: La humedad excesiva puede causar oxidación y problemas en componentes sensibles\n",
        "\n",
        "X₃ = Velocidad de la Línea de Producción\n",
        "\n",
        "Unidad: Unidades por hora (uph)\n",
        "Rango típico: 800 - 1,200 uph\n",
        "Justificación: Velocidades muy altas pueden comprometer la calidad por tiempo insuficiente de proceso\n",
        "\n",
        "X₄ = Tiempo desde el Último Mantenimiento Preventivo\n",
        "\n",
        "Unidad: Horas de operación\n",
        "Rango típico: 0 - 200 horas\n",
        "Justificación: Equipos con más tiempo sin mantenimiento tienden a generar más defectos\n",
        "\n",
        "X₅ = Experiencia del Operador del Turno\n",
        "\n",
        "Unidad: Años de experiencia\n",
        "Rango típico: 0.5 - 15 años\n",
        "Justificación: Operadores más experimentados detectan y corrigen problemas más rápidamente\n",
        "\n",
        "\n",
        "3. Tipo de Problema: REGRESIÓN\n",
        "Justificación:\n",
        "\n",
        "Variable dependiente numérica continua: La tasa de defectos es un porcentaje que puede tomar cualquier valor entre 0% y 100%\n",
        "Objetivo de predicción: Queremos predecir el valor específico de la tasa de defectos\n",
        "Interpretación: Un modelo de regresión nos permitirá predecir numéricamente qué tan alta será la tasa de defectos bajo diferentes condiciones"
      ],
      "metadata": {
        "id": "Kd6LwwzvU7H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 2: Preprocesamiento de Datos y *Leakage***\n",
        "\n",
        "Basado en el caso que definiste en la Parte 1:\n",
        "\n",
        "1.  Lista entre 3 y 5 **transformaciones** que aplicarías a tus datos (ej. imputación de valores faltantes, codificación de variables categóricas, escalado, creación de *lags*, etc.) y **justifica por qué** cada una es necesaria.\n",
        "2.  Señala un posible riesgo de ***data leakage*** (fuga de datos) en tu plan y explica cómo lo evitarías usando un *pipeline* de preprocesamiento.\n"
      ],
      "metadata": {
        "id": "d95Z8vhTU8Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Transformaciones de Datos\n",
        "Transformación 1: Imputación de Valores Faltantes\n",
        "Método: Imputación por mediana para variables numéricas y forward-fill para datos temporales\n",
        "Variables afectadas: Todas las X (especialmente temperatura y humedad por fallos de sensores)\n",
        "Justificación:\n",
        "\n",
        "Realidad operativa: Los sensores de temperatura y humedad pueden fallar temporalmente o generar lecturas erróneas\n",
        "Por qué mediana: Menos sensible a outliers que la media, común en datos de manufactura\n",
        "Forward-fill: Para datos temporales, el último valor válido es una aproximación razonable para períodos cortos"
      ],
      "metadata": {
        "id": "zOpOk7KVVBAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para valores faltantes esporádicos\n",
        "# Assuming 'temperatura' is a column in your 'data' DataFrame\n",
        "data['temperatura_filled'] = data['temperatura'].fillna(data['temperatura'].median())\n",
        "\n",
        "# Para secuencias temporales cortas\n",
        "# Assuming 'humedad' is a column in your 'data' DataFrame\n",
        "data['humedad_filled'] = data['humedad'].fillna(method='ffill', limit=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Okmb1YmcthWu",
        "outputId": "1cd19049-1cc1-438b-fd5b-02e13c22ab0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1255652157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Para valores faltantes esporádicos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Assuming 'temperatura' is a column in your 'data' DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperatura_filled'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperatura'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperatura'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Para secuencias temporales cortas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación 2: Detección y Tratamiento de Outliers\n",
        "Método: Método IQR (Rango Intercuartílico) con winsorización\n",
        "Variables afectadas: Temperatura, velocidad de línea, humedad\n",
        "Justificación:\n",
        "\n",
        "Outliers comunes: Temperaturas extremas por mal funcionamiento, velocidades anómalas por paradas de emergencia\n",
        "Impacto en modelos: Los outliers pueden sesgar significativamente modelos de regresión\n",
        "Winsorización vs eliminación: Preservamos el tamaño del dataset reemplazando valores extremos por percentiles"
      ],
      "metadata": {
        "id": "m9qOc2gctjv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detectar outliers con IQR\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "# Winsorizar a percentiles 5-95\n",
        "data_clean = data.clip(lower=data.quantile(0.05), upper=data.quantile(0.95))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HS2gbImKtnjJ",
        "outputId": "6f3af486-d8aa-4837-bb9a-33db5be5cc51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1406641315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Detectar outliers con IQR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mQ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mIQR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Winsorizar a percentiles 5-95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación 3: Escalado/Normalización (StandardScaler)\n",
        "Método: Estandarización Z-score\n",
        "Variables afectadas: Temperatura (°C), Humedad (%), Velocidad (uph), Tiempo mantenimiento (horas)\n",
        "Justificación:\n",
        "\n",
        "Diferentes escalas: Temperatura (~240°C), Humedad (~50%), Velocidad (~1000 uph)\n",
        "Algoritmos sensibles: Regresión lineal, SVM, redes neuronales requieren variables en escala similar\n",
        "Interpretación de coeficientes: Permite comparar la importancia relativa de variables"
      ],
      "metadata": {
        "id": "Jif1KVSdtqal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_numeric)"
      ],
      "metadata": {
        "id": "3io881h9ts-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación 4: Creación de Variables de Interacción y Lags\n",
        "Variables nuevas:\n",
        "\n",
        "Lag de temperatura: Temperatura del lote anterior\n",
        "Interacción temperatura-humedad: Producto de temperatura normalizada × humedad normalizada\n",
        "Ventana móvil de defectos: Promedio de defectos de los últimos 3 lotes\n",
        "\n",
        "Justificación:\n",
        "\n",
        "Memoria del proceso: Los defectos actuales pueden depender de condiciones previas\n",
        "Efectos combinados: Temperatura + humedad alta pueden tener efecto multiplicativo en defectos\n",
        "Tendencias: El promedio móvil captura tendencias que un punto individual no muestra"
      ],
      "metadata": {
        "id": "Fc4VU8altv4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag features\n",
        "data['temp_lag1'] = data['temperatura'].shift(1)\n",
        "# Interacciones\n",
        "data['temp_humidity_interaction'] = data['temperatura'] * data['humedad']\n",
        "# Ventana móvil\n",
        "data['defects_rolling_3'] = data['tasa_defectos'].shift(1).rolling(3).mean()"
      ],
      "metadata": {
        "id": "L8Z_d875tyUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación 5: Codificación de Variable Categórica (One-Hot Encoding)\n",
        "Variable: Turno de trabajo (Mañana, Tarde, Noche)\n",
        "Método: One-Hot Encoding\n",
        "Justificación:\n",
        "\n",
        "Variable categórica ordinal: Diferentes turnos pueden tener diferentes niveles de defectos\n",
        "No asumir orden: One-hot evita asumir que \"Noche\" > \"Tarde\" > \"Mañana\"\n",
        "Flexibilidad del modelo: Permite que cada turno tenga su propio coeficiente independiente"
      ],
      "metadata": {
        "id": "dpotrnw9tzeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "turno_encoded = encoder.fit_transform(data[['turno']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "AQw92j0Ut3vj",
        "outputId": "93b3a382-61b9-45d8-e404-592563be9d00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1003317312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mturno_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'turno'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Riesgo de Data Leakage y Prevención\n",
        "Riesgo Identificado: Leakage Temporal\n",
        "Problema específico: Usar información futura para predecir el pasado en la creación de ventanas móviles\n",
        "Ejemplo del error:"
      ],
      "metadata": {
        "id": "44KxDXdmt99w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ❌ INCORRECTO - Data Leakage\n",
        "data['defects_rolling_3'] = data['tasa_defectos'].rolling(3).mean()\n",
        "# Esto usa información del presente para \"predecir\" el presente"
      ],
      "metadata": {
        "id": "dycN_UoDuE1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por qué es problemático:\n",
        "\n",
        "En producción: No tendremos la tasa de defectos actual para predecir la tasa de defectos actual\n",
        "Overfitting artificial: El modelo parece mejor de lo que realmente es\n",
        "Falla en implementación: El modelo no funcionará en producción real\n",
        "\n",
        "Solución: Pipeline de Preprocesamiento Robusto"
      ],
      "metadata": {
        "id": "nvW86d8IuFqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "class SafeTimeSeriesTransformer:\n",
        "    def __init__(self):\n",
        "        self.scalers = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Solo usar datos históricos para calcular estadísticas\n",
        "        for col in X.columns:\n",
        "            if col.endswith('_lag') or col == 'temp_humidity_interaction':\n",
        "                continue\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            self.scalers[col].fit(X[col].values.reshape(-1, 1))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "\n",
        "        # ✅ CORRECTO - Solo usar información pasada\n",
        "        X_transformed['temp_lag1'] = X['temperatura'].shift(1)\n",
        "        X_transformed['defects_rolling_3'] = X['tasa_defectos'].shift(1).rolling(3).mean()\n",
        "        X_transformed['temp_humidity_interaction'] = X['temperatura'] * X['humedad']\n",
        "\n",
        "        # Escalar usando estadísticas del conjunto de entrenamiento\n",
        "        for col, scaler in self.scalers.items():\n",
        "            if col in X_transformed.columns:\n",
        "                X_transformed[col] = scaler.transform(X_transformed[col].values.reshape(-1, 1)).flatten()\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "# Pipeline completo\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('time_features', SafeTimeSeriesTransformer()),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# División temporal correcta\n",
        "def temporal_split(data, train_ratio=0.7):\n",
        "    split_point = int(len(data) * train_ratio)\n",
        "    return data[:split_point], data[split_point:]\n",
        "\n",
        "# Uso correcto\n",
        "train_data, test_data = temporal_split(data)\n",
        "\n",
        "# Fit solo en datos de entrenamiento\n",
        "preprocessing_pipeline.fit(train_data.drop('tasa_defectos', axis=1))\n",
        "\n",
        "# Transform por separado\n",
        "X_train_processed = preprocessing_pipeline.transform(train_data.drop('tasa_defectos', axis=1))\n",
        "X_test_processed = preprocessing_pipeline.transform(test_data.drop('tasa_defectos', axis=1))"
      ],
      "metadata": {
        "id": "MsWsGw73uI5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principios Clave para Evitar Leakage:\n",
        "\n",
        "División temporal estricta: Entrenar solo con datos anteriores al período de prueba\n",
        "Estadísticas solo del entrenamiento: Medias, desviaciones, etc., calculadas solo en train\n",
        "Lags apropiados: Usar siempre .shift(1) o más para variables target\n",
        "Validación temporal: Cross-validation que respete el orden temporal\n",
        "Pipeline consistente: Mismas transformaciones en entrenamiento y producción\n",
        "\n",
        "Validación del pipeline:"
      ],
      "metadata": {
        "id": "FrRHollauLjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que no hay leakage\n",
        "assert X_train_processed.shape[0] == train_data.shape[0]\n",
        "assert not X_train_processed.isnull().any().any()  # Sin valores faltantes\n",
        "assert X_test_processed.mean().abs().max() < 3     # Escalado correcto"
      ],
      "metadata": {
        "id": "fI3tXMofuQPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este enfoque garantiza que el modelo entrenado será replicable en producción, evitando la trampa común del data leakage temporal que puede hacer que un modelo parezca excelente en desarrollo pero falle completamente en implementación real."
      ],
      "metadata": {
        "id": "BFYWzJ_iuSXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 3: Interpretación y Métricas de Regresión Simple**\n",
        "\n",
        "Para esta sección, elige un caso simple de regresión (puede ser el tuyo o uno hipotético, como predecir la demanda de un producto según su precio).\n",
        "\n",
        "1.  Define claramente las variables **$Y$** y **$X$** junto con sus **unidades** (ej. $Y$: número de unidades vendidas, $X$: precio en dólares).\n",
        "1. Definición de Variables\n",
        "Variable Dependiente (Y):\n",
        "\n",
        "Y = Unidades vendidas por semana\n",
        "Unidad: Número de unidades (entero positivo)\n",
        "Rango típico: 50 - 500 unidades\n",
        "\n",
        "Variable Independiente (X):\n",
        "\n",
        "X = Precio de venta por unidad\n",
        "Unidad: Dólares estadounidenses ($USD)\n",
        "Rango típico: $15 - $45 por unidad\n",
        "\n",
        "Contexto: Una tienda de electrónicos quiere predecir cuántas unidades de auriculares bluetooth venderá semanalmente según el precio que establezca.\n",
        "2.  Supón que entrenas un modelo y obtienes una pendiente de **$\\hat{\\beta}_1 = -0.6$**. Escribe una interpretación clara y concisa de este coeficiente en el contexto de tu problema.\n",
        "Modelo resultante:\n",
        "Unidades Vendidas = β₀ + (-0.6) × Precio\n",
        "Interpretación:\n",
        "\n",
        "\"Por cada dólar adicional que aumentemos el precio, esperamos vender 0.6 unidades menos por semana, manteniendo todo lo demás constante.\"\n",
        "\n",
        "Ejemplos prácticos:\n",
        "\n",
        "Si el precio actual es $25 y lo subimos a $30 (+$5), esperamos vender 3 unidades menos (5 × 0.6 = 3)\n",
        "Si bajamos el precio de $35 a $30 (-$5), esperamos vender 3 unidades más\n",
        "\n",
        "Significado económico:\n",
        "\n",
        "El coeficiente negativo confirma la ley básica de la demanda (precio ↑ → demanda ↓)\n",
        "La magnitud (-0.6) indica una sensibilidad moderada al precio\n",
        "Es una relación inelástica en términos unitarios (< 1 en valor absoluto)\n",
        "3.  ¿Qué **métrica** de evaluación usarías (MAE, RMSE, o MAPE) y **por qué** es la más adecuada para tu caso?\n",
        "Fórmula:\n",
        "MAPE = (1/n) × Σ |((Y_real - Y_predicho) / Y_real)| × 100\n",
        "¿Por qué MAPE es la más adecuada?\n",
        "1. Interpretabilidad para el negocio:\n",
        "\n",
        "MAPE = 15% significa que nuestras predicciones se desvían en promedio 15% del valor real\n",
        "Los gerentes entienden fácilmente los porcentajes vs. unidades absolutas\n",
        "\n",
        "2. Escala independiente:\n",
        "\n",
        "Permite comparar modelos entre diferentes productos con volúmenes distintos\n",
        "Un error de 10 unidades es muy diferente si vendemos 50 vs. 500 unidades\n",
        "\n",
        "3. Relevancia comercial:\n",
        "\n",
        "En retail, los errores porcentuales son más relevantes para decisiones de inventario\n",
        "Un error del 20% tiene implicaciones similares independientemente del volumen base\n",
        "\n",
        "Comparación con otras métricas:\n",
        "MAE (Mean Absolute Error):\n",
        "\n",
        "Ventaja: Fácil interpretación en unidades originales\n",
        "Desventaja: No considera la escala relativa (error de 10 unidades es diferente para productos de alta vs. baja rotación)\n",
        "\n",
        "RMSE (Root Mean Square Error):\n",
        "\n",
        "Ventaja: Penaliza más los errores grandes\n",
        "Desventaja: Menos interpretable para stakeholders no técnicos, sensible a outliers\n",
        "4.  Menciona **un supuesto** del modelo de regresión lineal que validarías (ej. linealidad, homocedasticidad) y explica **cómo** lo harías (usando un gráfico o una prueba estadística).\n",
        "Supuesto Seleccionado: Homocedasticidad\n",
        "Definición: La variabilidad de los residuos debe ser constante a lo largo de todos los valores predichos.\n",
        "¿Por qué es crítico en este caso?\n",
        "\n",
        "Si la variabilidad cambia con el precio, nuestras predicciones serán menos confiables en ciertos rangos\n",
        "Violación común: Mayor variabilidad en ventas cuando los precios son muy bajos o muy altos\n",
        "\n",
        "Métodos de Validación:\n",
        "Método 1: Gráfico de Residuos vs. Valores Predichos"
      ],
      "metadata": {
        "id": "apwleKVOVGM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Calcular residuos\n",
        "residuos = y_real - y_predicho\n",
        "\n",
        "# Crear gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_predicho, residuos, alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Valores Predichos (Unidades)')\n",
        "plt.ylabel('Residuos')\n",
        "plt.title('Residuos vs. Valores Predichos - Verificación de Homocedasticidad')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BRWkU6pMxgtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación visual:\n",
        "\n",
        "Homocedasticidad: Puntos distribuidos aleatoriamente alrededor de y=0 con varianza constante\n",
        "Heterocedasticidad: Patrón de embudo (varianza creciente/decreciente)"
      ],
      "metadata": {
        "id": "mhSjrIm-xl5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "# Realizar prueba\n",
        "lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(residuos, X_design_matrix)\n",
        "\n",
        "print(f\"Estadístico LM: {lm:.4f}\")\n",
        "print(f\"P-valor: {lm_pvalue:.4f}\")\n",
        "\n",
        "# Interpretación\n",
        "if lm_pvalue < 0.05:\n",
        "    print(\"Rechazamos H₀: Evidencia de heterocedasticidad\")\n",
        "else:\n",
        "    print(\"No rechazamos H₀: No hay evidencia suficiente de heterocedasticidad\")"
      ],
      "metadata": {
        "id": "N-s-mLMLxtUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 4: Regresión Múltiple y Colinealidad**\n",
        "\n",
        "Volviendo a tu caso de la Parte 1 (con múltiples variables).\n",
        "\n",
        "1.  Escribe el **vector de variables** $\\vec{X}$ y la **respuesta** $Y$.\n",
        "2.  Explica cómo interpretarías el **coeficiente** de una de tus variables clave (incluyendo unidades y el sentido de la relación: positiva o negativa).\n",
        "3.  Si sospecharas que existe **colinealidad** entre tus variables, menciona **dos acciones** que podrías tomar para mitigarla."
      ],
      "metadata": {
        "id": "ekoLxjHVVL1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Vector de Variables y RespuestaVector X⃗:\n",
        "X⃗ = [X₁, X₂, X₃, X₄, X₅]Donde:\n",
        "\n",
        "X₁ = Temperatura del proceso de soldadura (°C)\n",
        "X₂ = Humedad relativa del ambiente (%)\n",
        "X₃ = Velocidad de la línea de producción (unidades/hora)\n",
        "X₄ = Tiempo desde último mantenimiento preventivo (horas)\n",
        "X₅ = Experiencia del operador del turno (años)\n",
        "Variable Respuesta Y:\n",
        "Y = Tasa de defectos por lote (%)Modelo de Regresión Múltiple:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + β₄X₄ + β₅X₅ + ε2. Interpretación del Coeficiente de Variable ClaveVariable seleccionada: X₄ (Tiempo desde último mantenimiento)Supongamos que obtenemos: β₄ = +0.025Interpretación completa:\n",
        "\"Por cada hora adicional que transcurre desde el último mantenimiento preventivo, esperamos que la tasa de defectos aumente en 0.025 puntos porcentuales, manteniendo constantes la temperatura, humedad, velocidad de línea y experiencia del operador.\"\n",
        "Desglose detallado:Unidades:\n",
        "\n",
        "Cambio en X₄: +1 hora\n",
        "Cambio esperado en Y: +0.025 puntos porcentuales\n",
        "Ratio de cambio: 0.025 puntos porcentuales por hora\n",
        "Sentido de la relación:\n",
        "\n",
        "Positiva: β₄ = +0.025 > 0\n",
        "Lógica: A medida que pasa más tiempo sin mantenimiento, los equipos se deterioran y generan más defectos\n",
        "Ejemplos prácticos:\n",
        "Escenario base: Mantenimiento recién realizado (X₄ = 0 horas)\n",
        "\n",
        "Tasa de defectos predicha: Y_base\n",
        "\n",
        "\n",
        "\n",
        "Después de 40 horas: X₄ = 40 horas\n",
        "\n",
        "Incremento esperado: 40 × 0.025 = 1.0 punto porcentual\n",
        "Nueva tasa predicha: Y_base + 1.0%\n",
        "\n",
        "\n",
        "\n",
        "Después de 100 horas: X₄ = 100 horas\n",
        "\n",
        "Incremento esperado: 100 × 0.025 = 2.5 puntos porcentuales\n",
        "Nueva tasa predicha: Y_base + 2.5%\n",
        "\n",
        "\n",
        "Significado operacional:\n",
        "\n",
        "Riesgo gradual: Los defectos aumentan progresivamente, no abruptamente\n",
        "Punto de intervención: Si el coeficiente es significativo, ayuda a determinar intervalos óptimos de mantenimiento\n",
        "Costo-beneficio: Balancear costo de mantenimiento vs. costo de defectos adicionales\n",
        "3. Mitigación de ColinealidadColinealidad esperada en mi caso:Pares problemáticos potenciales:\n",
        "\n",
        "X₁ y X₂: Temperatura y humedad pueden estar inversamente correlacionadas (aire caliente retiene menos humedad)\n",
        "X₃ y X₄: Velocidad alta puede acelerar el desgaste, correlacionándose con tiempo de mantenimiento\n",
        "X₄ y Y (variable latente): Tiempo de mantenimiento podría correlacionarse con vibraciones o desgaste no observado\n",
        "Acción 1: Análisis de Matriz de Correlación y VIFDiagnóstico:"
      ],
      "metadata": {
        "id": "2Scm1jCXVOLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Matriz de correlación\n",
        "correlation_matrix = X_df.corr()\n",
        "print(\"Correlaciones altas (|r| > 0.7):\")\n",
        "high_corr = np.where(np.abs(correlation_matrix) > 0.7)\n",
        "for i, j in zip(*high_corr):\n",
        "    if i != j:\n",
        "        print(f\"{X_df.columns[i]} - {X_df.columns[j]}: {correlation_matrix.iloc[i,j]:.3f}\")\n",
        "\n",
        "# Factor de Inflación de Varianza (VIF)\n",
        "X_with_constant = sm.add_constant(X_df)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X_with_constant.columns[1:]  # Excluir constante\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_constant.values, i+1)\n",
        "                   for i in range(len(X_with_constant.columns)-1)]\n",
        "\n",
        "print(\"Variables con VIF > 5 (colinealidad problemática):\")\n",
        "print(vif_data[vif_data[\"VIF\"] > 5])"
      ],
      "metadata": {
        "id": "IzzWRrwxyDho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criterios de decisión:\n",
        "\n",
        "VIF > 10: Colinealidad severa, acción requerida\n",
        "VIF 5-10: Colinealidad moderada, monitorear\n",
        "|r| > 0.8: Correlación alta entre variables\n",
        "\n",
        "Solución específica:\n",
        "Si encontramos X₁ (Temperatura) y X₂ (Humedad) altamente correlacionadas:"
      ],
      "metadata": {
        "id": "a-6vSaBwyKE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar la variable menos importante teóricamente\n",
        "X_reduced = X_df.drop(['humedad'], axis=1)\n",
        "# O crear una variable combinada\n",
        "X_df['indice_comfort_termico'] = X_df['temperatura'] / X_df['humedad']"
      ],
      "metadata": {
        "id": "thBtAb4ayMDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acción 2: Regularización Ridge\n",
        "Implementación:"
      ],
      "metadata": {
        "id": "cdGMX0nlyMnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir rango de valores alpha para regularización\n",
        "alpha_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "# Grid search para encontrar alpha óptimo\n",
        "ridge_model = Ridge()\n",
        "param_grid = {'alpha': alpha_range}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    ridge_model,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "grid_search.fit(X_scaled, y)\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "print(f\"Alpha óptimo: {best_alpha}\")\n",
        "\n",
        "# Modelo final con regularización\n",
        "ridge_final = Ridge(alpha=best_alpha)\n",
        "ridge_final.fit(X_scaled, y)"
      ],
      "metadata": {
        "id": "VlSRInvYyPAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ventajas de Ridge en este contexto:\n",
        "1. Contracción de coeficientes:\n",
        "\n",
        "Reduce la magnitud de coeficientes correlacionados\n",
        "Distribuye el \"efecto\" entre variables colineales\n",
        "\n",
        "2. Estabilidad predictiva:\n",
        "\n",
        "Reduce la varianza de las predicciones\n",
        "Mejor generalización en datos nuevos\n",
        "\n",
        "3. Mantiene todas las variables:\n",
        "\n",
        "A diferencia de Lasso, no elimina variables completamente\n",
        "Preserva interpretabilidad operacional\n",
        "\n",
        "Comparación de coeficientes:"
      ],
      "metadata": {
        "id": "MvMo9N09yWNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparar coeficientes antes y después de regularización\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Variable': X_df.columns,\n",
        "    'OLS': ols_model.coef_,\n",
        "    'Ridge': ridge_final.coef_\n",
        "})\n",
        "\n",
        "coef_comparison.set_index('Variable').plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Comparación de Coeficientes: OLS vs Ridge')\n",
        "plt.ylabel('Valor del Coeficiente')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QC7FtHqIyW5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estrategia Integral:\n",
        "1. Prevención durante recolección de datos:\n",
        "\n",
        "Diseñar experimentos que minimicen correlaciones naturales\n",
        "Recopilar datos en diferentes condiciones operativas\n",
        "\n",
        "2. Monitoreo continuo:\n",
        "\n",
        "Calcular VIF regularmente con nuevos datos\n",
        "Establecer alertas automáticas cuando VIF > 5\n",
        "\n",
        "3. Validación del impacto:\n",
        "\n",
        "Comparar R² y métricas de error antes/después de mitigación\n",
        "Verificar que la interpretabilidad se mantenga para stakeholders\n",
        "\n",
        "4. Documentación operacional:\n",
        "\n",
        "Registrar decisiones de eliminación/combinación de variables\n",
        "Mantener trazabilidad para auditorías de calidad\n",
        "\n",
        "Esta aproximación sistemática asegura que el modelo mantenga tanto precisión predictiva como interpretabilidad operacional, elementos críticos en un ambiente de manufactura donde las decisiones deben ser explicables y accionables."
      ],
      "metadata": {
        "id": "YIE5_s6nyZwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 5: Interacciones y Multicolinealidad (VIF)**\n",
        "\n",
        "1.  Plantea un caso con una variable $Y$ y entre 4 y 6 variables $X$. ¿Qué término de **interacción** entre dos variables podrías añadir al modelo y **por qué** crees que sería útil?\n",
        "2.  Si al calcular el Factor de Inflación de la Varianza (VIF) para una variable, obtienes un valor alto (ej. > 10), menciona **dos acciones** que podrías tomar para solucionarlo."
      ],
      "metadata": {
        "id": "hT8kIG_JVOmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Definición del Modelo con InteracciónVariables del Modelo:Variable Dependiente:\n",
        "\n",
        "Y = Puntuación de Satisfacción (escala 1-10)\n",
        "Variables Independientes:\n",
        "\n",
        "X₁ = Calidad de la comida (escala 1-10)\n",
        "X₂ = Velocidad del servicio (minutos desde pedido hasta entrega)\n",
        "X₃ = Precio promedio del plato (dólares)\n",
        "X₄ = Ambiente del restaurante (escala 1-10: ruido, iluminación, decoración)\n",
        "X₅ = Experiencia del mesero (años de experiencia)\n",
        "X₆ = Día de la semana (1=Lunes, 7=Domingo)\n",
        "Término de Interacción Propuesto: X₁ × X₂Modelo con interacción:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + β₄X₄ + β₅X₅ + β₆X₆ + β₇(X₁ × X₂) + εDonde: X₁ × X₂ = Calidad de comida × Velocidad del servicio¿Por qué esta interacción es útil?Razón Conceptual:\n",
        "\n",
        "\"El efecto de la calidad de la comida en la satisfacción del cliente depende de qué tan rápido se sirve, y viceversa.\"\n",
        "Escenarios prácticos:1. Comida excelente + Servicio rápido (X₁=9, X₂=15 min):\n",
        "\n",
        "Efecto amplificado: La experiencia excepcional se potencia\n",
        "Satisfacción esperada: Muy alta\n",
        "2. Comida excelente + Servicio lento (X₁=9, X₂=45 min):\n",
        "\n",
        "Efecto atenuado: La espera frustra al cliente pese a la buena comida\n",
        "Satisfacción esperada: Moderada\n",
        "3. Comida regular + Servicio rápido (X₁=6, X₂=15 min):\n",
        "\n",
        "Compensación parcial: La rapidez compensa la calidad promedio\n",
        "Satisfacción esperada: Aceptable\n",
        "4. Comida regular + Servicio lento (X₁=6, X₂=45 min):\n",
        "\n",
        "Efecto negativo combinado: Ambos aspectos decepcionantes\n",
        "Satisfacción esperada: Baja\n",
        "Interpretación del coeficiente β₇:Si β₇ = -0.008:\n",
        "\n",
        "\"Por cada minuto adicional de espera, el efecto positivo de la calidad de la comida en la satisfacción se reduce en 0.008 puntos por cada punto de calidad.\"\n",
        "Ejemplo numérico:\n",
        "\n",
        "Comida calidad 8, servicio 20 min: Efecto interacción = 8 × 20 × (-0.008) = -1.28 puntos\n",
        "Comida calidad 8, servicio 40 min: Efecto interacción = 8 × 40 × (-0.008) = -2.56 puntos\n",
        "Diferencia: 20 minutos extra de espera reducen 1.28 puntos adicionales la satisfacción\n",
        "Valor para el negocio:\n",
        "\n",
        "Estrategia operativa: Identificar el balance óptimo calidad-velocidad\n",
        "Gestión de expectativas: En días ocupados, comunicar tiempos de espera\n",
        "Capacitación de personal: Priorizar rapidez cuando la calidad sea consistente\n",
        "Pricing dinámico: Ajustar precios según la combinación calidad-servicio ofrecida\n",
        "2. Manejo de VIF Alto (VIF > 10)Ejemplo: Variable X₃ (Precio) con VIF = 15.2Este VIF alto indica que X₃ está altamente correlacionado con otras variables del modelo, lo que genera problemas de multicolinealidad.Acción 1: Eliminación Estratégica de VariablesProceso sistemático:Paso 1: Diagnosticar correlaciones específicas"
      ],
      "metadata": {
        "id": "1EfI4MuqVT0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Identificar correlaciones altas\n",
        "correlation_matrix = X_df.corr()\n",
        "high_correlations = correlation_matrix['precio'].abs().sort_values(ascending=False)\n",
        "print(\"Correlaciones de 'precio' con otras variables:\")\n",
        "print(high_correlations[high_correlations > 0.7])\n",
        "\n",
        "# Ejemplo de resultado:\n",
        "# ambiente         0.85  <- Restaurantes caros suelen tener mejor ambiente\n",
        "# experiencia_mesero 0.78  <- Restaurantes caros contratan meseros experimentados"
      ],
      "metadata": {
        "id": "L0XlLH_HyyFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2: Decidir qué variable eliminar"
      ],
      "metadata": {
        "id": "M_RmllG9y084"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular VIF para todas las variables correlacionadas\n",
        "variables_problema = ['precio', 'ambiente', 'experiencia_mesero']\n",
        "vif_subset = []\n",
        "\n",
        "for var in variables_problema:\n",
        "    X_temp = X_df[variables_problema]\n",
        "    X_temp = sm.add_constant(X_temp)\n",
        "    vif = variance_inflation_factor(X_temp.values,\n",
        "                                   variables_problema.index(var) + 1)\n",
        "    vif_subset.append((var, vif))\n",
        "\n",
        "print(\"VIF para variables correlacionadas:\")\n",
        "for var, vif in vif_subset:\n",
        "    print(f\"{var}: {vif:.2f}\")"
      ],
      "metadata": {
        "id": "9lQpSR2Sy2vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Criterios de eliminación\n",
        "\n",
        "Importancia teórica: ¿Cuál es más crítica para el negocio?\n",
        "Poder predictivo individual: ¿Cuál tiene mayor correlación con Y?\n",
        "Facilidad de medición: ¿Cuál es más fácil/barata de obtener?\n",
        "\n",
        "Decisión ejemplo:"
      ],
      "metadata": {
        "id": "qU25H-DXy7BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar 'experiencia_mesero' porque:\n",
        "# 1. Información más difícil de obtener\n",
        "# 2. 'ambiente' y 'precio' son más directos para estrategia de negocio\n",
        "X_reduced = X_df.drop(['experiencia_mesero'], axis=1)\n",
        "\n",
        "# Recalcular VIF\n",
        "X_temp = sm.add_constant(X_reduced)\n",
        "vif_nuevos = []\n",
        "for i in range(1, X_temp.shape[1]):  # Excluir constante\n",
        "    vif = variance_inflation_factor(X_temp.values, i)\n",
        "    vif_nuevos.append((X_temp.columns[i], vif))\n",
        "\n",
        "print(\"VIF después de eliminación:\")\n",
        "for var, vif in vif_nuevos:\n",
        "    print(f\"{var}: {vif:.2f}\")"
      ],
      "metadata": {
        "id": "mIUeaArjy9Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acción 2: Combinación/Transformación de Variables\n",
        "Estrategia: Crear variables compuestas\n",
        "Opción A: Índice de Valor Percibido"
      ],
      "metadata": {
        "id": "ZoHcXUVrzAS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar precio y ambiente en un índice de \"relación calidad-precio\"\n",
        "# Normalizar variables primero\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "precio_norm = scaler.fit_transform(X_df[['precio']])\n",
        "ambiente_norm = scaler.fit_transform(X_df[['ambiente']])\n",
        "\n",
        "# Crear índice de valor: ambiente/precio (mayor = mejor valor)\n",
        "X_df['indice_valor'] = ambiente_norm.flatten() / (precio_norm.flatten() + 0.1)  # +0.1 para evitar división por 0\n",
        "\n",
        "# Eliminar variables originales problemáticas\n",
        "X_combinado = X_df.drop(['precio', 'ambiente'], axis=1)"
      ],
      "metadata": {
        "id": "-kBYfWzlzCqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción B: Análisis de Componentes Principales (PCA)"
      ],
      "metadata": {
        "id": "twSZj5I9zEpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Aplicar PCA solo a variables correlacionadas\n",
        "variables_correlacionadas = ['precio', 'ambiente', 'experiencia_mesero']\n",
        "X_subset = X_df[variables_correlacionadas]\n",
        "\n",
        "# Normalizar antes de PCA\n",
        "X_subset_scaled = StandardScaler().fit_transform(X_subset)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components=2)  # Reducir de 3 a 2 componentes\n",
        "componentes_pca = pca.fit_transform(X_subset_scaled)\n",
        "\n",
        "# Crear nuevas variables\n",
        "X_df['componente_lujo'] = componentes_pca[:, 0]  # Primer componente\n",
        "X_df['componente_servicio'] = componentes_pca[:, 1]  # Segundo componente\n",
        "\n",
        "# Verificar varianza explicada\n",
        "print(f\"Varianza explicada por componentes: {pca.explained_variance_ratio_}\")\n",
        "# Ejemplo: [0.68, 0.23] = 91% de varianza original preservada\n",
        "\n",
        "# Eliminar variables originales\n",
        "X_pca_final = X_df.drop(variables_correlacionadas, axis=1)"
      ],
      "metadata": {
        "id": "14hHDWBqzFN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validación de efectividad:\n",
        "1. Verificar VIF reducido:"
      ],
      "metadata": {
        "id": "JTQ9gC9XzIF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalcular VIF para modelo transformado\n",
        "X_final = sm.add_constant(X_combinado)  # o X_pca_final\n",
        "vif_final = []\n",
        "for i in range(1, X_final.shape[1]):\n",
        "    vif = variance_inflation_factor(X_final.values, i)\n",
        "    vif_final.append((X_final.columns[i], vif))\n",
        "\n",
        "print(\"VIF después de combinación:\")\n",
        "for var, vif in vif_final:\n",
        "    print(f\"{var}: {vif:.2f}\")"
      ],
      "metadata": {
        "id": "5GvV2BGGzKTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Comparar poder predictivo:"
      ],
      "metadata": {
        "id": "4CIFTPJ6zOmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Modelo original (con multicolinealidad)\n",
        "modelo_original = LinearRegression()\n",
        "scores_original = cross_val_score(modelo_original, X_df, y, cv=5, scoring='r2')\n",
        "\n",
        "# Modelo reducido\n",
        "modelo_reducido = LinearRegression()\n",
        "scores_reducido = cross_val_score(modelo_reducido, X_combinado, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"R² promedio original: {scores_original.mean():.4f} (+/- {scores_original.std()*2:.4f})\")\n",
        "print(f\"R² promedio reducido: {scores_reducido.mean():.4f} (+/- {scores_reducido.std()*2:.4f})\")"
      ],
      "metadata": {
        "id": "jokm3YFrzRAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criterios de Selección entre Acciones:\n",
        "Usar eliminación cuando:\n",
        "\n",
        "Una variable es claramente redundante\n",
        "El poder predictivo se mantiene\n",
        "La interpretabilidad es crítica\n",
        "\n",
        "Usar combinación cuando:\n",
        "\n",
        "Todas las variables aportan información única\n",
        "Se puede crear un índice con significado de negocio\n",
        "La pérdida de interpretabilidad es aceptable\n",
        "\n",
        "Resultado esperado: VIF < 5 para todas las variables, manteniendo al menos 85% del poder predictivo original y preservando la interpretabilidad operativa para decisiones de negocio."
      ],
      "metadata": {
        "id": "VPIwJeBPzVEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 6: Variables Categóricas e Interacciones**\n",
        "\n",
        "1.  Define una **variable categórica** para tu caso (puedes inventarla si no la tenías). Elige una de sus categorías como el nivel **base** o de referencia y **justifica** tu elección.\n",
        "2.  Crea una **interacción** entre una variable numérica y la variable categórica que definiste. Explica cómo se interpretaría el coeficiente de esta interacción."
      ],
      "metadata": {
        "id": "1Sn14YcNVUzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable Categórica: Tipo de Restaurante\n",
        "Retomando el caso de Predicción de Satisfacción del Cliente en Restaurantes, voy a definir una variable categórica adicional:\n",
        "1. Variable Categórica Definida\n",
        "Variable: X₇ = Tipo de Restaurante\n",
        "Categorías:\n",
        "\n",
        "Casual (Fast-casual, cafeterías)\n",
        "Familiar (Restaurantes de barrio, cadenas familiares)\n",
        "Fino (Fine dining, alta cocina)\n",
        "\n",
        "Nivel Base Seleccionado: \"Familiar\"\n",
        "Justificación de la elección:\n",
        "1. Representatividad estadística:\n",
        "\n",
        "Los restaurantes familiares constituyen aproximadamente 50-60% del mercado\n",
        "Mayor tamaño de muestra garantiza estimaciones más estables\n",
        "Reduce la varianza de los coeficientes estimados\n",
        "\n",
        "2. Punto de referencia lógico:\n",
        "\n",
        "Representa el \"estándar medio\" del mercado\n",
        "Facilita comparaciones: restaurantes casuales vs. familiares, finos vs. familiares\n",
        "Las expectativas de clientes en restaurantes familiares son más predecibles\n",
        "\n",
        "3. Interpretabilidad para el negocio:\n",
        "\n",
        "Los gerentes pueden entender fácilmente desviaciones desde esta base\n",
        "Permite estrategias de posicionamiento claras (subir hacia \"fino\" o bajar hacia \"casual\")\n",
        "\n",
        "Codificación Dummy resultante:\n",
        "X₇_Casual = 1 si es Casual, 0 si no\n",
        "X₇_Fino = 1 si es Fino, 0 si no\n",
        "(Familiar = 0, 0 - referencia implícita)\n",
        "Modelo extendido:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + β₄X₄ + β₅X₅ + β₆X₆ +\n",
        "    β₇X₇_Casual + β₈X₇_Fino + ε\n",
        "Interpretación de coeficientes:\n",
        "\n",
        "β₇: Diferencia promedio en satisfacción entre restaurantes Casuales vs. Familiares\n",
        "β₈: Diferencia promedio en satisfacción entre restaurantes Finos vs. Familiares\n",
        "\n",
        "\n",
        "2. Interacción: Variable Numérica × Categórica\n",
        "Interacción Propuesta: X₃ (Precio) × X₇ (Tipo de Restaurante)\n",
        "Modelo con interacción:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + β₄X₄ + β₅X₅ + β₆X₆ +\n",
        "    β₇X₇_Casual + β₈X₇_Fino +\n",
        "    β₉(X₃ × X₇_Casual) + β₁₀(X₃ × X₇_Fino) + ε\n",
        "Términos de interacción:\n",
        "\n",
        "β₉: X₃ × X₇_Casual (Precio × Dummy_Casual)\n",
        "β₁₀: X₃ × X₇_Fino (Precio × Dummy_Fino)\n",
        "\n",
        "Interpretación de los Coeficientes de Interacción\n",
        "¿Por qué esta interacción es relevante?\n",
        "\n",
        "\"El efecto del precio en la satisfacción del cliente varía según el tipo de restaurante, porque las expectativas y sensibilidad al precio son diferentes en cada segmento.\"\n",
        "\n",
        "Interpretación detallada de β₉ (Precio × Casual):\n",
        "Supongamos β₉ = -0.15\n",
        "\n",
        "\"En restaurantes casuales, por cada dólar adicional en el precio promedio, la satisfacción disminuye 0.15 puntos más de lo que disminuiría en restaurantes familiares.\"\n",
        "\n",
        "Desglose matemático:\n",
        "Para Restaurantes Familiares (base):\n",
        "Efecto del precio = β₃\n",
        "Para Restaurantes Casuales:\n",
        "Efecto del precio = β₃ + β₉ = β₃ + (-0.15)\n",
        "Ejemplo numérico (β₃ = -0.10):\n",
        "\n",
        "Familiares: Por cada $1 de aumento, satisfacción baja 0.10 puntos\n",
        "Casuales: Por cada $1 de aumento, satisfacción baja 0.25 puntos (0.10 + 0.15)\n",
        "\n",
        "Interpretación detallada de β₁₀ (Precio × Fino):\n",
        "Supongamos β₁₀ = +0.08\n",
        "\n",
        "\"En restaurantes finos, por cada dólar adicional en el precio promedio, la satisfacción disminuye 0.08 puntos menos de lo que disminuiría en restaurantes familiares.\"\n",
        "\n",
        "Para Restaurantes Finos:\n",
        "Efecto del precio = β₃ + β₁₀ = -0.10 + 0.08 = -0.02\n",
        "Comparación de sensibilidades al precio:\n",
        "\n",
        "Casuales: -0.25 puntos por dólar (muy sensibles)\n",
        "Familiares: -0.10 puntos por dólar (sensibilidad media)\n",
        "Finos: -0.02 puntos por dólar (poco sensibles)\n",
        "\n",
        "Interpretación de Negocio\n",
        "¿Por qué estos coeficientes tienen sentido?\n",
        "1. Restaurantes Casuales (β₉ = -0.15):\n",
        "\n",
        "Clientes sensibles al precio: Buscan valor y conveniencia\n",
        "Expectativas claras: Precio bajo = experiencia aceptable\n",
        "Elasticidad alta: Aumentos de precio generan insatisfacción desproporcionada\n",
        "\n",
        "2. Restaurantes Finos (β₁₀ = +0.08):\n",
        "\n",
        "Clientes menos sensibles: Priorizan calidad sobre precio\n",
        "Expectativa premium: Precio alto = calidad excepcional esperada\n",
        "Elasticidad baja: Aumentos de precio no afectan tanto la satisfacción\n",
        "\n",
        "3. Restaurantes Familiares (base):\n",
        "\n",
        "Sensibilidad moderada: Balance entre precio y calidad\n",
        "Expectativas balanceadas: Precio justo por valor recibido\n",
        "\n",
        "Predicciones Específicas por Segmento\n",
        "Ejemplo: Plato de $25\n",
        "Restaurante Casual:\n",
        "Contribución del precio = β₃ × 25 + β₉ × 25 × 1\n",
        "                       = -0.10 × 25 + (-0.15) × 25\n",
        "                       = -2.5 - 3.75 = -6.25 puntos\n",
        "Restaurante Familiar:\n",
        "Contribución del precio = β₃ × 25 = -0.10 × 25 = -2.5 puntos\n",
        "Restaurante Fino:\n",
        "Contribución del precio = β₃ × 25 + β₁₀ × 25 × 1\n",
        "                       = -0.10 × 25 + 0.08 × 25\n",
        "                       = -2.5 + 2.0 = -0.5 puntos\n",
        "Implicaciones Estratégicas\n",
        "Para Restaurantes Casuales:\n",
        "\n",
        "Pricing muy cuidadoso: Pequeños aumentos pueden causar grandes pérdidas de satisfacción\n",
        "Enfoque en eficiencia: Mantener costos bajos para ofrecer precios atractivos\n",
        "Valor percibido: Comunicar claramente el valor de la propuesta\n",
        "\n",
        "Para Restaurantes Finos:\n",
        "\n",
        "Flexibilidad en pricing: Mayor margen para aumentos de precio\n",
        "Inversión en calidad: Los clientes toleran precios altos si la calidad es consistente\n",
        "Premium positioning: Mantener percepción de exclusividad\n",
        "\n",
        "Para Restaurantes Familiares:\n",
        "\n",
        "Estrategia balanceada: Equilibrio entre precio competitivo y calidad\n",
        "Diferenciación: Encontrar nichos que justifiquen precio vs. casuales\n",
        "\n",
        "Validación del Modelo de Interacción"
      ],
      "metadata": {
        "id": "C6s2LqX0VYoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "# Crear variables dummy\n",
        "df['casual'] = (df['tipo_restaurante'] == 'Casual').astype(int)\n",
        "df['fino'] = (df['tipo_restaurante'] == 'Fino').astype(int)\n",
        "\n",
        "# Crear términos de interacción\n",
        "df['precio_x_casual'] = df['precio'] * df['casual']\n",
        "df['precio_x_fino'] = df['precio'] * df['fino']\n",
        "\n",
        "# Ajustar modelo\n",
        "X = df[['calidad_comida', 'velocidad_servicio', 'precio', 'ambiente',\n",
        "        'experiencia_mesero', 'dia_semana', 'casual', 'fino',\n",
        "        'precio_x_casual', 'precio_x_fino']]\n",
        "X = sm.add_constant(X)\n",
        "y = df['satisfaccion']\n",
        "\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# Test de significancia de interacciones\n",
        "f_test = model.f_test(['precio_x_casual = 0', 'precio_x_fino = 0'])\n",
        "print(f\"F-test para interacciones: {f_test}\")"
      ],
      "metadata": {
        "id": "7rSV7FpMzvnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta interacción revela cómo las diferentes expectativas y comportamientos de clientes según el tipo de restaurante modifican fundamentalmente la relación precio-satisfacción, proporcionando insights accionables para estrategias de pricing diferenciadas por segmento."
      ],
      "metadata": {
        "id": "zBD2S2CnzzfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 7: Conceptos Clave de Clasificación**\n",
        "\n",
        "Aunque el taller se centra en regresión, estos conceptos son fundamentales en Machine Learning.\n",
        "\n",
        "1.  Explica qué es la **curva ROC** y para qué se utiliza en un problema de clasificación.\n",
        "\n",
        "¿Qué es la Curva ROC?\n",
        "La curva ROC (Receiver Operating Characteristic) es una herramienta gráfica fundamental para evaluar el rendimiento de modelos de clasificación binaria. ROC significa \"Característica Operativa del Receptor\", un término que proviene de la teoría de detección de señales desarrollada durante la Segunda Guerra Mundial para analizar señales de radar.\n",
        "Definición y Componentes\n",
        "La curva ROC es un gráfico bidimensional que representa la relación entre:\n",
        "\n",
        "Eje Y (Sensibilidad): Tasa de Verdaderos Positivos (TPR = True Positive Rate)\n",
        "Eje X (1-Especificidad): Tasa de Falsos Positivos (FPR = False Positive Rate)\n",
        "\n",
        "Fórmulas clave:\n",
        "\n",
        "TPR (Sensibilidad) = TP / (TP + FN)\n",
        "FPR (1-Especificidad) = FP / (FP + TN)\n",
        "\n",
        "Donde:\n",
        "\n",
        "TP = Verdaderos Positivos\n",
        "FP = Falsos Positivos\n",
        "TN = Verdaderos Negativos\n",
        "FN = Falsos Negativos\n",
        "\n",
        "¿Para qué se utiliza la Curva ROC?\n",
        "1. Evaluación de Rendimiento del Modelo\n",
        "La curva ROC permite visualizar qué tan bien un modelo de clasificación binaria puede distinguir entre las dos clases. Un modelo perfecto tendría una curva que va directamente hacia la esquina superior izquierda (TPR = 1, FPR = 0).\n",
        "2. Comparación de Modelos\n",
        "Permite comparar múltiples algoritmos de clasificación en un mismo gráfico. El modelo con la curva más cercana a la esquina superior izquierda generalmente tiene mejor rendimiento.\n",
        "3. Selección del Punto de Corte Óptimo\n",
        "Cada punto en la curva ROC representa un umbral de decisión diferente. Esto ayuda a seleccionar el punto de corte que mejor equilibre sensibilidad y especificidad según las necesidades del problema.\n",
        "4. Cálculo del Área Bajo la Curva (AUC)\n",
        "El AUC-ROC es una métrica única que resume el rendimiento del modelo:\n",
        "\n",
        "AUC = 1: Modelo perfecto\n",
        "AUC = 0.5: Modelo equivalente al azar\n",
        "AUC < 0.5: Modelo peor que el azar (puede invertirse)\n",
        "\n",
        "Interpretación Práctica\n",
        "Formas típicas de la curva:\n",
        "Curva ideal: Se acerca a la esquina superior izquierda, indicando alta sensibilidad con baja tasa de falsos positivos.\n",
        "Línea diagonal: Representa un clasificador aleatorio (AUC = 0.5), sin capacidad discriminativa.\n",
        "Curva por debajo de la diagonal: Indica un modelo que está haciendo predicciones inversas a las correctas.\n",
        "Ventajas de la Curva ROC\n",
        "\n",
        "Independent del umbral: Evalúa el modelo en todos los posibles puntos de corte\n",
        "Invariante a la escala: No se ve afectada por la escala de las probabilidades predichas\n",
        "Comparación visual clara: Permite comparar fácilmente múltiples modelos\n",
        "Métrica única (AUC): Proporciona un valor numérico para comparaciones objetivas\n",
        "\n",
        "Limitaciones Importantes\n",
        "\n",
        "Problemas con datasets desbalanceados: En casos de clases muy desproporcionadas, la curva ROC puede ser demasiado optimista\n",
        "No considera costos: No tiene en cuenta si los falsos positivos o falsos negativos tienen costos diferentes\n",
        "Solo para clasificación binaria: Requiere adaptaciones para problemas multiclase\n",
        "\n",
        "Cuándo Usar ROC vs Otras Métricas\n",
        "Usar ROC cuando:\n",
        "\n",
        "Las clases están relativamente balanceadas\n",
        "Los costos de FP y FN son similares\n",
        "Se necesita evaluar el modelo en diferentes umbrales\n",
        "\n",
        "Considerar alternativas cuando:\n",
        "\n",
        "Hay un gran desbalance de clases (usar curva Precision-Recall)\n",
        "Los costos de errores son muy diferentes\n",
        "Se necesita una métrica más interpretable para stakeholders no técnicos\n",
        "\n",
        "\n",
        "2.  Define el concepto de **accuracy** (exactitud) y menciona una situación en la que podría ser una métrica engañosa.\n",
        "¿Qué es la Accuracy (Exactitud)?\n",
        "La accuracy o exactitud es la métrica más básica e intuitiva para evaluar modelos de clasificación. Mide la proporción de predicciones correctas que hace el modelo sobre el total de predicciones realizadas.\n",
        "Fórmula:\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "Donde:\n",
        "\n",
        "TP = Verdaderos Positivos (predicciones correctas de la clase positiva)\n",
        "TN = Verdaderos Negativos (predicciones correctas de la clase negativa)\n",
        "FP = Falsos Positivos (predicciones incorrectas como positivo)\n",
        "FN = Falsos Negativos (predicciones incorrectas como negativo)\n",
        "\n",
        "Interpretación:\n",
        "\n",
        "Valor: Entre 0 y 1 (o 0% y 100%)\n",
        "Interpretación: Porcentaje de casos clasificados correctamente\n",
        "Ejemplo: Accuracy = 0.85 significa que el modelo acierta en el 85% de los casos\n",
        "\n",
        "¿Cuándo la Accuracy Puede Ser Engañosa?\n",
        "Situación Crítica: Datasets Desbalanceados\n",
        "La accuracy se vuelve una métrica profundamente engañosa cuando trabajamos con clases desbalanceadas, donde una clase es mucho más frecuente que la otra.\n",
        "Ejemplo Práctico: Detección de Fraude Bancario\n",
        "Imagina un modelo para detectar transacciones fraudulentas con las siguientes características:\n",
        "Dataset:\n",
        "\n",
        "Transacciones legítimas: 9,950 casos (99.5%)\n",
        "Transacciones fraudulentas: 50 casos (0.5%)\n",
        "Total: 10,000 transacciones\n",
        "\n",
        "Modelo \"Ingenuo\":\n",
        "Un modelo que simplemente predice \"NO FRAUDE\" para todos los casos tendría:\n",
        "\n",
        "Predicciones correctas: 9,950 (todas las legítimas)\n",
        "Predicciones incorrectas: 50 (todos los fraudes no detectados)\n",
        "Accuracy = 9,950/10,000 = 99.5%\n",
        "\n",
        "¿Por qué es Engañosa esta Accuracy?\n",
        "Aparenta excelente rendimiento: Un 99.5% de accuracy suena impresionante y podría llevar a creer que tenemos un modelo excelente.\n",
        "Realidad crítica: El modelo es completamente inútil porque:\n",
        "\n",
        "No detecta ningún fraude: 0% de detección en fraudes reales\n",
        "Costo empresarial enorme: Todas las transacciones fraudulentas pasan desapercibidas\n",
        "Falla en su propósito principal: El objetivo era detectar fraudes, no clasificar transacciones legítimas\n",
        "\n",
        "Otros Escenarios Donde la Accuracy Engaña\n",
        "1. Diagnóstico de Enfermedades Raras\n",
        "\n",
        "Dataset: 99% pacientes sanos, 1% con enfermedad rara\n",
        "Modelo que siempre dice \"sano\": 99% accuracy\n",
        "Problema: 0% de detección de la enfermedad que necesitamos diagnosticar\n",
        "\n",
        "2. Detección de Spam\n",
        "\n",
        "Dataset: 95% emails legítimos, 5% spam\n",
        "Modelo que nunca detecta spam: 95% accuracy\n",
        "Problema: Toda la bandeja se llena de spam\n",
        "\n",
        "3. Control de Calidad Industrial\n",
        "\n",
        "Dataset: 98% productos buenos, 2% defectuosos\n",
        "Modelo que aprueba todo: 98% accuracy\n",
        "Problema: Productos defectuosos llegan al cliente\n",
        "\n",
        "¿Por Qué Ocurre Este Problema?\n",
        "Dominancia de la Clase Mayoritaria\n",
        "La accuracy se ve dominada por la clase más frecuente. En datasets desbalanceados, acertar en la clase mayoritaria contribuye desproporcionalmente al resultado final.\n",
        "Insensibilidad al Propósito del Modelo\n",
        "La accuracy no distingue entre la importancia de detectar correctamente diferentes clases. En muchos casos prácticos, detectar la clase minoritaria es más crítico.\n",
        "Métricas Alternativas Más Apropiadas\n",
        "Para Datasets Desbalanceados:\n",
        "1. Precision (Precisión):\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "¿De las predicciones positivas, cuántas son correctas?\n",
        "\n",
        "2. Recall (Sensibilidad):\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "¿De los casos positivos reales, cuántos detectamos?\n",
        "\n",
        "3. F1-Score:\n",
        "\n",
        "F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "Media armónica entre precision y recall\n",
        "\n",
        "4. AUC-ROC:\n",
        "\n",
        "Área bajo la curva ROC\n",
        "Evalúa el rendimiento en todos los umbrales\n",
        "\n",
        "5. Balanced Accuracy:\n",
        "\n",
        "(Sensibilidad + Especificidad) / 2\n",
        "Promedia el rendimiento en ambas clases\n",
        "\n",
        "¿Cuándo Sí Usar Accuracy?\n",
        "La accuracy sigue siendo útil cuando:\n",
        "\n",
        "Clases balanceadas: Distribución similar entre clases\n",
        "Costos equivalentes: El costo de FP y FN es similar\n",
        "Objetivo general: Se busca rendimiento global sin priorizar ninguna clase\n",
        "Comparación inicial: Como primera aproximación para comparar modelos\n",
        "\n",
        "Recomendación Práctica\n",
        "Nunca uses accuracy como única métrica. Siempre complementa con:\n",
        "\n",
        "Análisis de la distribución de clases\n",
        "Matriz de confusión para ver errores específicos\n",
        "Métricas específicas según el problema (precision, recall, F1)\n",
        "Consideración del contexto de negocio y costos de errores\n",
        "\n",
        "La accuracy del 99.5% en el ejemplo de fraudes puede costar millones de dólares en pérdidas reales, mientras que un modelo con 85% accuracy pero que detecta 80% de los fraudes sería infinitamente más valioso. El contexto y propósito del modelo siempre deben guiar la selección de métricas de evaluación.\n",
        "\n",
        "3.  Describe qué es una **matriz de confusión** y cómo se interpretan sus componentes (Verdaderos Positivos, Falsos Positivos, Verdaderos Negativos, Falsos Negativos).\n",
        "¿Qué es una Matriz de Confusión?\n",
        "La matriz de confusión es una tabla de contingencia que proporciona una representación visual detallada del rendimiento de un modelo de clasificación. Es una herramienta fundamental que desglosa todas las predicciones del modelo comparándolas con los valores reales, permitiendo identificar exactamente dónde y cómo se está \"confundiendo\" el algoritmo.\n",
        "Estructura Básica (Clasificación Binaria)\n",
        "                    PREDICCIÓN\n",
        "                 Positivo  Negativo\n",
        "REAL Positivo      TP       FN\n",
        "     Negativo      FP       TN\n",
        "Donde:\n",
        "\n",
        "Filas: Clases reales (ground truth)\n",
        "Columnas: Predicciones del modelo\n",
        "Celdas: Cantidad de casos en cada combinación\n",
        "\n",
        "Componentes de la Matriz de Confusión\n",
        "1. Verdaderos Positivos (TP - True Positives)\n",
        "Definición: Casos donde el modelo predijo correctamente la clase positiva.\n",
        "\n",
        "Real: Positivo\n",
        "Predicción: Positivo\n",
        "Resultado:  CORRECTO\n",
        "\n",
        "Ejemplo práctico: En diagnóstico médico, pacientes que realmente tienen la enfermedad Y el modelo los diagnosticó como enfermos.\n",
        "2. Falsos Positivos (FP - False Positives)\n",
        "Definición: Casos donde el modelo predijo incorrectamente la clase positiva.\n",
        "\n",
        "Real: Negativo\n",
        "Predicción: Positivo\n",
        "Resultado:  ERROR (Falsa Alarma)\n",
        "\n",
        "Ejemplo práctico: Pacientes sanos que el modelo diagnosticó incorrectamente como enfermos. También llamado Error Tipo I.\n",
        "3. Verdaderos Negativos (TN - True Negatives)\n",
        "Definición: Casos donde el modelo predijo correctamente la clase negativa.\n",
        "\n",
        "Real: Negativo\n",
        "Predicción: Negativo\n",
        "Resultado:  CORRECTO\n",
        "\n",
        "Ejemplo práctico: Pacientes sanos que el modelo correctamente diagnosticó como sanos.\n",
        "4. Falsos Negativos (FN - False Negatives)\n",
        "Definición: Casos donde el modelo predijo incorrectamente la clase negativa.\n",
        "\n",
        "Real: Positivo\n",
        "Predicción: Negativo\n",
        "Resultado:  ERROR (Caso Perdido)\n",
        "\n",
        "Ejemplo práctico: Pacientes enfermos que el modelo no logró detectar, diagnosticándolos como sanos. También llamado Error Tipo II."
      ],
      "metadata": {
        "id": "7zVS_WjcVbuR"
      }
    }
  ]
}